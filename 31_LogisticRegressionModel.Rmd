```{r load-data}
library(here)

source(here("00_requirements.R")) 

load(here("processed_data", "cleaned_data.RData"))
```

```{r}
glimpse(cleaned_data)
```

Build a basic logistic regression model with hand-picked variables to have a "base" model for comparisons.

```{r}
cleaned_data_clean <- cleaned_data %>%
  clean_names()

cleaned_data_clean <- cleaned_data_clean %>%
  mutate(voting_results_2024 = as.factor(voting_results_2024))

cleaned_data_clean$voting_results_2024 <- relevel(cleaned_data_clean$voting_results_2024, ref = "D")

set.seed(123) 


data_split <- initial_split(cleaned_data_clean, prop = 0.75, strata = voting_results_2024)

train_data <- training(data_split)
test_data  <- testing(data_split)


model_logistic <- glm(
  voting_results_2024 ~ median_household_income + 
                        unemployment_rate_august_2025 + 
                        white + 
                        black + 
                        hispanic,
  data = train_data,
  family = "binomial"
)

# See the summary of the model (coefficients, p-values, etc.)
summary(model_logistic)

probabilities <- predict(model_logistic, newdata = test_data, type = "response")

predictions <- ifelse(probabilities > 0.5, "R", "D")

confusion_matrix <- table(
  Predicted = predictions, 
  Actual = test_data$voting_results_2024
)

print(confusion_matrix)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(paste("Model Accuracy on Test Data:", round(accuracy * 100, 2), "%"))

plot(model_logistic)
```

Backwards eliminiation:


```{r}

library(janitor)
library(rsample)
library(dplyr)

# 1. Clean names (Good practice)
cleaned_data_clean <- cleaned_data %>%
  clean_names()

# 2. Ensure the target is Numeric (0 and 1)
# We skip 'relevel' because 0 and 1 imply their own order (0 is baseline)
cleaned_data_clean <- cleaned_data_clean %>%
  mutate(voting_results_2024 = as.numeric(voting_results_2024))

set.seed(123) 

# 3. Split the data
data_split <- initial_split(cleaned_data_clean, prop = 0.75, strata = voting_results_2024)
train_data <- training(data_split)
test_data  <- testing(data_split)

# Step 1: Install the package that handles "perfect separation"
if(!require(arm)) install.packages("arm")
library(arm)

# Step 2: Use bayesglm instead of glm
# We also removed "White" because White + Black + Hispanic creates multicollinearity 
# (They sum to 100%, which confuses the math)
library(arm)

library(arm)

# Hypothesis: Guns (Republican) vs. Cities/Transit (Democrat)
urban_model <- bayesglm(
  voting_results_2024 ~ percentage_of_households_that_own_guns + 
  median_household_income + 
  public_transportation,   # <--- The "City" proxy
  data = train_data, 
  family = "binomial"
)

summary(urban_model)

# Predictions
probabilities <- predict(urban_model, newdata = test_data, type = "response")
predictions <- ifelse(probabilities > 0.5, "D", "R")

# Confusion Matrix
actuals_labels <- ifelse(test_data$voting_results_2024 == 1, "D", "R")
confusion_matrix <- table(Predicted = predictions, Actual = actuals_labels)
print(confusion_matrix)

# Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Model Accuracy:", round(accuracy * 100, 2), "%"))

```


```{r}

library(blorr)

# 1. Define the full model using standard glm (not bayesglm)
full_glm_model <- glm(
  voting_results_2024 ~ percentage_of_households_that_own_guns + 
  median_household_income + 
  worked_at_home + 
  black + hispanic + unemployment_rate_august_2025,
  data = train_data, 
  family = "binomial"
)

# 2. Run the Backward Selection based on P-values
# prem = 0.05 means "Remove variable if p-value is > 0.05"
selection <- blr_step_p_backward(full_glm_model, prem = 0.05)

# 3. View the results
selection
```






